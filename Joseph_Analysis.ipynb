{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install dynamax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QiM2MtwlDqC",
        "outputId": "99aca0cc-f613-486d-ba4f-b5a639b79ada"
      },
      "id": "4QiM2MtwlDqC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dynamax in /usr/local/lib/python3.9/dist-packages (0.1.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.9/dist-packages (from dynamax) (0.2.14)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.9/dist-packages (from dynamax) (1.0.3)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (from dynamax) (0.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from dynamax) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from dynamax) (4.5.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.9/dist-packages (from dynamax) (0.4.6+cuda11.cudnn86)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from dynamax) (0.4.6)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.9/dist-packages (from dynamax) (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->dynamax) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->dynamax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->dynamax) (1.10.1)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping->dynamax) (3.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax->dynamax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax->dynamax) (0.1.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->dynamax) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->dynamax) (3.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dynamax) (1.16.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dynamax) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dynamax) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dynamax) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dynamax) (2.2.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->dynamax) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping->dynamax) (6.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping->dynamax) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "15e092f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15e092f7",
        "outputId": "815bb818-03d3-4c26-ca85-94d2e2842834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from dynamax.hidden_markov_model import CategoricalHMM\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax import vmap\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_numpy(composer,song):\n",
        "  main_dir = \"/content/Composers\"\n",
        "  df = pd.read_csv(main_dir + '/' + composer + '-output-csvs' + '/' + song)\n",
        "  return df[['note_num','normed_duration']].to_numpy()"
      ],
      "metadata": {
        "id": "CzBRn9aafovx"
      },
      "id": "CzBRn9aafovx",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dictionaries(files = ['train.pkl','test.pkl']):\n",
        "  train_file = files[0]\n",
        "  test_file = files[1]\n",
        "  print(train_file)\n",
        "  with open(train_file,'rb') as f:\n",
        "    train_dict = pickle.load(f)\n",
        "  with open(test_file,'rb') as f:\n",
        "    test_dict = pickle.load(f)\n",
        "\n",
        "  return train_dict, test_dict"
      ],
      "metadata": {
        "id": "OUGNJEs31stH"
      },
      "id": "OUGNJEs31stH",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, test_dict = load_dictionaries()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbME5Kv2DIk",
        "outputId": "f38fcd0a-6b24-441c-b9a7-ab3c6d40b677"
      },
      "id": "MZbME5Kv2DIk",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test():\n",
        "  main_dir = \"/content/Composers\"\n",
        "  train_dict = {}\n",
        "  test_dict = {}\n",
        "\n",
        "  # NOTE: deleted debussy and balakir for lack of data\n",
        "  percent_split = 0.8\n",
        "\n",
        "  for file_name in os.listdir(main_dir):\n",
        "    comp_list = []\n",
        "    if file_name[0] == '.':\n",
        "      continue\n",
        "    for song in os.listdir(main_dir + '/' + file_name):\n",
        "      comp_list.append(song)\n",
        "    np.random.shuffle(comp_list)  # shuffle list\n",
        "    # manual train-test split\n",
        "    len_dir = len(comp_list)\n",
        "    eighty = int(len_dir*percent_split)\n",
        "    if eighty < 1:\n",
        "      raise ValueError(\"Need to rewrite percentsplit!\")\n",
        "\n",
        "    train_dict[file_name.split('-')[0]] = comp_list[:eighty]\n",
        "    test_dict[file_name.split('-')[0]] = comp_list[eighty:]\n",
        "\n",
        "    filename = \"/content/train.pkl\"\n",
        "  with open(filename, 'wb') as f:\n",
        "    pickle.dump(train_dict, f)\n",
        "\n",
        "  filename = \"/content/test.pkl\"\n",
        "  with open(filename, 'wb') as f:\n",
        "    pickle.dump(test_dict, f)\n"
      ],
      "metadata": {
        "id": "WtlmOoaW2LxQ"
      },
      "id": "WtlmOoaW2LxQ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3d22464f",
      "metadata": {
        "id": "3d22464f"
      },
      "outputs": [],
      "source": [
        "def print_params(params):\n",
        "    jnp.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "    print(\"initial probs:\")\n",
        "    print(params.initial.probs)\n",
        "    print(\"transition matrix:\")\n",
        "    print(params.transitions.transition_matrix)\n",
        "    print(\"emission probs:\")\n",
        "    print(params.emissions.probs) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef920e72",
      "metadata": {
        "id": "ef920e72"
      },
      "source": [
        "Now Let's work with the test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ComposerAnalysis(comps_list, train_dict = train_dict, test_dict = test_dict,verbose = True):\n",
        "  \"\"\"function to analyze a list of composers using provided data in a folder at the directory:\n",
        "  /content/Composers. Each composers song data should be saved as a csv of the format outputted\n",
        "  by the midi-csv submodule script titled midi_to_csv.py.\n",
        "  PARAMETERS:\n",
        "\n",
        "    comps_list (list): list of the names of composers to run the analysis on as they appear in the\n",
        "        train dictionary and test dictionary\n",
        "    train_dict (dictionary): dictionary of composer names to a list of the names of the csvs that contain\n",
        "        the composer's training songs\n",
        "    test_dict (dictionary): dictionary of composer names to a list of the names of the csvs that contain that\n",
        "        composer's test songs\n",
        "    verbose (bool): whether or not to alert user to progress of program\n",
        "\n",
        "  RETURNS:\n",
        "    NONE, but prints the scoring of the test set for Nearest Neighbors and K-Means\n",
        "    \"\"\"\n",
        "  main_dir = \"/content/Composers\"\n",
        "  # create labels to be used to identify composers\n",
        "  comp_labels = {composer:i for i, composer in enumerate(comps_list)}\n",
        "\n",
        "  # 1) go through all the songs we are training on to get the number of unique notes and durations\n",
        "  # these will be used in the construction of the HMM model\n",
        "  notes = set()\n",
        "  durations = set()\n",
        "  test_songs = 0\n",
        "  train_songs = 0\n",
        "\n",
        "  for composer in comps_list:\n",
        "    for song in test_dict[composer]:\n",
        "      test_songs += 1\n",
        "      arr = csv_to_numpy(composer,song)\n",
        "      notes.update(arr[:,0])\n",
        "      durations.update(arr[:,1])\n",
        "    for song in train_dict[composer]:\n",
        "      train_songs += 1\n",
        "      arr = csv_to_numpy(composer,song)\n",
        "      notes.update(arr[:,0])\n",
        "      durations.update(arr[:,1])\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"The total number of notes found was:\\t\\t{len(notes)}\\nThe \"\n",
        "    f\"total number of durations found was:\\t{len(durations)}\\n\\n\")\n",
        "\n",
        "  # 2) Train the an HMM model for each song to get the transition matrix for each song\n",
        "\n",
        "  num_unique_notes = len(notes)                             # for calculating classes\n",
        "  num_unique_durations = len(durations)                     # for calculating classes\n",
        "  num_states = 3                                            # dimension of hidden state must be more than num_emmisions. eventually we will grid search this\n",
        "  num_emmisions = 2                                         # dimension of observations, two because we have note value and durations\n",
        "  num_classes = num_unique_notes*num_unique_durations       # total number of values the observation can take on \n",
        "  param_list_train = []                                     # place to store the parameter objects\n",
        "  train_labels = []                                         # labels to now what song was from what composer for\n",
        "                                                            #   clustering later\n",
        "  # now training\n",
        "  if verbose:\n",
        "    print(\"NOW STARTING TRAINING\\n\")\n",
        "    print(f\"\\tTotal training songs: {train_songs}\\n\")\n",
        "\n",
        "  song_progress = 1\n",
        "\n",
        "  for composer in comps_list:\n",
        "    for song in train_dict[composer]:\n",
        "      if verbose:\n",
        "        print(f\"processing song {song_progress}/{train_songs}\")\n",
        "      song_progress += 1\n",
        "      train_labels.append(comp_labels[composer])\n",
        "      arr = csv_to_numpy(composer,song)\n",
        "      if (np.isnan(arr).any()) or (np.isinf(arr).any()):\n",
        "          print(\"NaN or Inf\")\n",
        "          print(np.isnan(arr).any())\n",
        "          print(np.isinf(arr).any())\n",
        "          print(\" at i =\", i)\n",
        "          continue\n",
        "      hmm = CategoricalHMM(num_states,num_emmisions,num_classes)        # create object\n",
        "      params, props = hmm.initialize(method=\"prior\")  \n",
        "      params, log_probs = hmm.fit_em(params,props,arr,num_iters=10)     # fiting training points\n",
        "      param_list_train.append(params)                                   # saving parameters for that song\n",
        "\n",
        "  # 3) Processing training data to be used in training classification models\n",
        "  if verbose:\n",
        "    print(\"PROCESSING TRAINING DATA\\n\")\n",
        "\n",
        "  train_Fmatrix_list = []\n",
        "  for params in param_list_train:                                       # extracting transition matrices (F matrices)\n",
        "    train_Fmatrix_list.append(params.transitions.transition_matrix)\n",
        "\n",
        "  X_for_training = []   \n",
        "  for matrix in train_Fmatrix_list:                                          # raveling transition matrices so that each\n",
        "      X_for_training.append(np.ravel(matrix))                           # can act as a point in the clustering algorithm\n",
        "\n",
        "\n",
        "  # 4) Training classification models with processed training data\n",
        "  if verbose:\n",
        "    print(\"TRAINING CLASSIFICATION MODELS\\n\")\n",
        "\n",
        "  NC = NearestCentroid()                                                # nearest neighbors classifier\n",
        "  NC.fit(np.array(X_for_training), train_labels)\n",
        "  KM = KMeans(3)                                                        # KMeans classifier\n",
        "  KM.fit(np.array(X_for_training), train_labels)\n",
        "\n",
        "\n",
        "  # 5) Get the test data by getting transition matrices by running the HMM model on the test points\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"STARTING TEST DATA GENERATION\\n\")\n",
        "    print(f\"\\tTotal test songs: {test_songs}\\n\")\n",
        "\n",
        "  param_list_test = []  \n",
        "  test_labels = []\n",
        "  song_progress = 0           # to keep track of how much progress in the for loop you've done\n",
        "\n",
        "  for composer in comps_list:\n",
        "    for song in test_dict[composer]:\n",
        "      if verbose:\n",
        "        print(f\"processing test song {song_progress}/{test_songs}\")\n",
        "      song_progress += 1\n",
        "      test_labels.append(comp_labels[composer])\n",
        "      arr = csv_to_numpy(composer,song)\n",
        "      if (np.isnan(arr).any()) or (np.isinf(arr).any()):\n",
        "          print(\"NaN or Inf\")\n",
        "          print(np.isnan(arr).any())\n",
        "          print(np.isinf(arr).any())\n",
        "          print(\" at i =\", i)\n",
        "          continue\n",
        "      hmm = CategoricalHMM(num_states,num_emmisions,num_classes)\n",
        "      params, props = hmm.initialize(method=\"prior\")\n",
        "      params, log_probs = hmm.fit_em(params,props,arr,num_iters=10)\n",
        "      param_list_test.append(params)\n",
        "\n",
        "  if verbose:\n",
        "    print(\"PROCESSING TEST DATA\\n\")\n",
        "\n",
        "  t_matrix_list_test = []\n",
        "  for params in param_list_test:                # raveling for clustering purposes\n",
        "      t_matrix_list_test.append(params.transitions.transition_matrix)\n",
        "  \n",
        "  X_for_scoring = []                            # place to store ravelled t-matrices of test data for scoring\n",
        "  for matrix in t_matrix_list_test:\n",
        "      X_for_scoring.append(np.ravel(matrix))\n",
        "\n",
        "  # 6) # Get scores based on the test data\n",
        "  if verbose:\n",
        "    print(\"SCORING PROCESSED TEST DATA\\n\")\n",
        "\n",
        "  NN_score = NC.score(np.array(X_for_scoring), test_labels)\n",
        "  KM_score = KM.score(np.array(X_for_scoring), test_labels)\n",
        "\n",
        "  print(f\"RESULTS:\\n\\nNearest Neighbors Score:\\t{NN_score}\\nK-Means Score:\\t\\t{KM_score}\")\n"
      ],
      "metadata": {
        "id": "n_ZmHRYNfKpf"
      },
      "id": "n_ZmHRYNfKpf",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comps_list = ['haydn','mozart','schubert']\n",
        "#ComposerAnalysis(comps_list)"
      ],
      "metadata": {
        "id": "DhBCoi8kpcC8"
      },
      "id": "DhBCoi8kpcC8",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_list = ['chopin','borodin','beeth','liszt','mendelssohn','schumann']"
      ],
      "metadata": {
        "id": "P_HkIws3wh_G"
      },
      "id": "P_HkIws3wh_G",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ComposerAnalysis(new_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "ngFEDVGHw2pY",
        "outputId": "7715c6de-7766-4a53-bbe9-fa0e085e562c"
      },
      "id": "ngFEDVGHw2pY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d92fcec87866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mComposerAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ComposerAnalysis' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZH4Arjj3Ono"
      },
      "id": "FZH4Arjj3Ono",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}